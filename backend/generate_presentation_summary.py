#!/usr/bin/env python3
"""
Generate a presentation-ready summary from demo test results
Creates formatted output perfect for slides and judge presentations
"""

import json
import sys
from datetime import datetime
from pathlib import Path

def load_test_results():
    """Load the demo test results."""
    report_file = Path("demo_test_report.json")
    
    if not report_file.exists():
        print("âŒ No demo test report found. Please run the demo first:")
        print("   python run_demo.py")
        return None
    
    with open(report_file, 'r') as f:
        return json.load(f)

def generate_presentation_summary(data):
    """Generate presentation-ready summary."""
    
    summary = data['summary']
    results = data['detailed_results']
    
    print("=" * 80)
    print("ðŸŽ¯ AGENTIC VOICE ASSISTANT - PRESENTATION SUMMARY")
    print("=" * 80)
    print(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Executive Summary
    print("ðŸ“Š EXECUTIVE SUMMARY")
    print("-" * 40)
    print(f"âœ… Success Rate: {summary['success_rate']:.1f}%")
    print(f"âš¡ Avg Response Time: {summary['average_response_time']:.2f} seconds")
    print(f"ðŸŽ¯ Routing Accuracy: {summary['average_routing_accuracy']*100:.1f}%")
    print(f"ðŸ“ˆ Avg Confidence: {summary['average_confidence']:.1%}")
    print(f"ðŸ§ª Tests Completed: {summary['successful_tests']}/{summary['total_tests']}")
    print()
    
    # Performance Highlights
    print("ðŸ† KEY PERFORMANCE HIGHLIGHTS")
    print("-" * 40)
    
    # Find fastest response
    fastest = min(results, key=lambda x: x.get('total_time', float('inf')))
    print(f"âš¡ Fastest Response: {fastest['total_time']:.3f}s ({fastest['question_type']})")
    
    # Find highest confidence
    highest_conf = max(results, key=lambda x: x.get('overall_confidence', 0))
    print(f"ðŸŽ¯ Highest Confidence: {highest_conf['overall_confidence']:.1%} ({highest_conf['question_type']})")
    
    # Count multi-agent queries
    multi_agent = [r for r in results if len(r.get('agents_used', [])) > 2]
    print(f"ðŸ¤– Multi-Agent Coordination: {len(multi_agent)} complex queries handled")
    
    # Response time categories
    fast = len([r for r in results if r.get('total_time', 0) < 1.0])
    print(f"ðŸŸ¢ Sub-second Responses: {fast}/{len(results)} ({fast/len(results)*100:.1f}%)")
    print()
    
    # Detailed Test Results
    print("ðŸ“‹ DETAILED TEST RESULTS")
    print("-" * 40)
    
    for i, result in enumerate(results, 1):
        if 'error' in result:
            print(f"{i}. âŒ {result['question_type']} - FAILED")
            continue
            
        print(f"{i}. {result['question_type']}")
        print(f"   â“ Question: {result['question'][:60]}...")
        print(f"   â±ï¸  Time: {result['total_time']:.3f}s")
        print(f"   ðŸ¤– Agents: {', '.join(result['agents_used'])}")
        print(f"   ðŸ“ˆ Confidence: {result['overall_confidence']:.1%}")
        print(f"   ðŸŽ¯ Routing: {result['routing_accuracy']*100:.1f}% accurate")
        
        if result.get('escalation_required'):
            print(f"   ðŸš¨ Escalation: Required")
        print()
    
    # Technology Showcase
    print("ðŸ”§ TECHNOLOGY SHOWCASE")
    print("-" * 40)
    print("âœ… Multi-Agent AI Architecture")
    print("   â€¢ Supervisor Agent for intelligent routing")
    print("   â€¢ Specialized Ticket and Knowledge agents")
    print("   â€¢ Dynamic agent coordination")
    print()
    print("âœ… Real-Time Performance")
    print(f"   â€¢ Average response: {summary['average_response_time']:.2f}s")
    print("   â€¢ Concurrent agent processing")
    print("   â€¢ Performance monitoring & optimization")
    print()
    print("âœ… Intelligent Query Understanding")
    print(f"   â€¢ {summary['average_routing_accuracy']*100:.1f}% routing accuracy")
    print("   â€¢ Intent classification and context awareness")
    print("   â€¢ Natural language processing")
    print()
    print("âœ… Enterprise Integration Ready")
    print("   â€¢ AWS Bedrock AI integration")
    print("   â€¢ Scalable microservices architecture")
    print("   â€¢ Real-time WebSocket communication")
    print()
    
    # Judge Talking Points
    print("ðŸŽ¤ KEY TALKING POINTS FOR JUDGES")
    print("-" * 40)
    print("1. INNOVATION:")
    print("   â€¢ Multi-agent architecture enables specialized expertise")
    print("   â€¢ Real-time voice interaction with sub-second responses")
    print("   â€¢ Intelligent routing reduces query resolution time")
    print()
    print("2. TECHNICAL EXCELLENCE:")
    print(f"   â€¢ {summary['success_rate']:.1f}% success rate demonstrates reliability")
    print(f"   â€¢ {summary['average_routing_accuracy']*100:.1f}% routing accuracy shows smart decision-making")
    print("   â€¢ Scalable cloud-native architecture")
    print()
    print("3. BUSINESS IMPACT:")
    print("   â€¢ Reduces support ticket resolution time")
    print("   â€¢ Improves customer satisfaction with instant responses")
    print("   â€¢ Scales human expertise through AI agents")
    print()
    print("4. MARKET READINESS:")
    print("   â€¢ Built on enterprise AWS infrastructure")
    print("   â€¢ Real-time performance suitable for production")
    print("   â€¢ Extensible agent framework for domain expansion")
    print()

def generate_slide_data(data):
    """Generate data formatted for presentation slides."""
    
    summary = data['summary']
    results = data['detailed_results']
    
    slide_data = {
        "performance_metrics": {
            "success_rate": f"{summary['success_rate']:.1f}%",
            "avg_response_time": f"{summary['average_response_time']:.2f}s",
            "routing_accuracy": f"{summary['average_routing_accuracy']*100:.1f}%",
            "avg_confidence": f"{summary['average_confidence']:.1%}"
        },
        "test_breakdown": [
            {
                "type": r['question_type'],
                "time": f"{r['total_time']:.3f}s",
                "agents": len(r.get('agents_used', [])),
                "confidence": f"{r['overall_confidence']:.1%}"
            }
            for r in results if 'error' not in r
        ],
        "agent_usage": {
            "SupervisorAgent": len([r for r in results if 'SupervisorAgent' in r.get('agents_used', [])]),
            "TicketAgent": len([r for r in results if 'TicketAgent' in r.get('agents_used', [])]),
            "KnowledgeAgent": len([r for r in results if 'KnowledgeAgent' in r.get('agents_used', [])])
        }
    }
    
    # Save slide data
    with open('presentation_slide_data.json', 'w') as f:
        json.dump(slide_data, f, indent=2)
    
    print("ðŸ’¾ Slide data saved to: presentation_slide_data.json")

def main():
    """Main function to generate presentation summary."""
    
    data = load_test_results()
    if not data:
        return
    
    generate_presentation_summary(data)
    generate_slide_data(data)
    
    print("ðŸŽ‰ Presentation summary generated successfully!")
    print("ðŸ“‹ Use this output for your judge presentation")

if __name__ == "__main__":
    main()